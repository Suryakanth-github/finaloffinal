{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1f7a43-f301-40cc-abd1-e8ac9895f3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: typing_extensions in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a9b89a21-e0ea-44a0-977a-7a2a55df4b8d/lib/python3.10/site-packages (4.13.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad1f017-228d-415e-8bad-4142d2a33cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.lang.IllegalStateException: jupyter client is not available because the python kernel is not defined. The kernel may be restarting or the repl may have been shut down.\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$getJupyterKernelListener$1(JupyterDriverLocal.scala:305)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$getJupyterKernelListener(JupyterDriverLocal.scala:304)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.executePython(JupyterDriverLocal.scala:813)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:701)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$37(DriverLocal.scala:1043)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$26(DriverLocal.scala:1034)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:80)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:981)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1422)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:668)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.lang.IllegalStateException: jupyter client is not available because the python kernel is not defined. The kernel may be restarting or the repl may have been shut down.\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.$anonfun$getJupyterKernelListener$1(JupyterDriverLocal.scala:305)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.com$databricks$backend$daemon$driver$JupyterDriverLocal$$getJupyterKernelListener(JupyterDriverLocal.scala:304)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.executePython(JupyterDriverLocal.scala:813)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:701)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$37(DriverLocal.scala:1043)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$26(DriverLocal.scala:1034)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:80)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:80)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:981)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1422)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:668)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "IllegalStateException: jupyter client is not available because the python kernel is not defined. The kernel may be restarting or the repl may have been shut down.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36af4a87-96f8-441c-823d-6943531d0cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Python kernel...\n"
     ]
    }
   ],
   "source": [
    "# Run this cell first to initialize the Python kernel\n",
    "print(\"Initializing Python kernel...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f3c665-de00-4300-8bdb-97a8afff236c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: datasets in /databricks/python3/lib/python3.10/site-packages (2.13.1)\nRequirement already satisfied: librosa in /databricks/python3/lib/python3.10/site-packages (0.10.0)\nCollecting torchaudio\n  Using cached torchaudio-2.6.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (1.1.1)\nCollecting tensorflow\n  Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\nCollecting mlflow\n  Using cached mlflow-2.21.2-py3-none-any.whl (28.2 MB)\nRequirement already satisfied: accelerate in /databricks/python3/lib/python3.10/site-packages (0.20.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (1.21.5)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.3.2)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from datasets) (1.4.4)\nRequirement already satisfied: multiprocess in /databricks/python3/lib/python3.10/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /databricks/python3/lib/python3.10/site-packages (from datasets) (8.0.0)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /databricks/python3/lib/python3.10/site-packages (from datasets) (2022.7.1)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: xxhash in /databricks/python3/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.1.1 in /databricks/python3/lib/python3.10/site-packages (from librosa) (4.3.0)\nRequirement already satisfied: lazy-loader>=0.1 in /databricks/python3/lib/python3.10/site-packages (from librosa) (0.3)\nRequirement already satisfied: numba>=0.51.0 in /databricks/python3/lib/python3.10/site-packages (from librosa) (0.55.1)\nRequirement already satisfied: soxr>=0.3.2 in /databricks/python3/lib/python3.10/site-packages (from librosa) (0.3.6)\nRequirement already satisfied: soundfile>=0.12.1 in /databricks/python3/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: audioread>=2.1.9 in /databricks/python3/lib/python3.10/site-packages (from librosa) (3.0.0)\nRequirement already satisfied: scipy>=1.2.0 in /databricks/python3/lib/python3.10/site-packages (from librosa) (1.9.1)\nRequirement already satisfied: pooch>=1.0 in /databricks/python3/lib/python3.10/site-packages (from librosa) (1.7.0)\nRequirement already satisfied: joblib>=0.14 in /databricks/python3/lib/python3.10/site-packages (from librosa) (1.2.0)\nRequirement already satisfied: decorator>=4.3.0 in /databricks/python3/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: msgpack>=1.0 in /databricks/python3/lib/python3.10/site-packages (from librosa) (1.0.5)\nCollecting torch==2.6.0\n  Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\nCollecting nvidia-nvjitlink-cu12==12.4.127\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\nCollecting nvidia-curand-cu12==10.3.5.147\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch==2.6.0->torchaudio) (2.8.4)\nCollecting nvidia-cuda-cupti-cu12==12.4.127\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch==2.6.0->torchaudio) (2.11.3)\nCollecting nvidia-cusolver-cu12==11.6.1.9\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\nCollecting sympy==1.13.1\n  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\nCollecting nvidia-cusparselt-cu12==0.6.2\n  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\nCollecting triton==3.2.0\n  Using cached triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\nCollecting typing-extensions>=4.1.1\n  Using cached typing_extensions-4.13.0-py3-none-any.whl (45 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\nCollecting nvidia-nvtx-cu12==12.4.127\n  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\nCollecting nvidia-nccl-cu12==2.21.5\n  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\nCollecting nvidia-cusparse-cu12==12.3.1.170\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\nCollecting nvidia-cufft-cu12==11.2.1.3\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.48.1)\nRequirement already satisfied: google-pasta>=0.1.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (63.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nCollecting tensorboard~=2.19.0\n  Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\nCollecting flatbuffers>=24.3.25\n  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\nCollecting ml-dtypes<1.0.0,>=0.5.1\n  Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\nCollecting h5py>=3.11.0\n  Using cached h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (0.33.0)\nRequirement already satisfied: libclang>=13.0.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (15.0.6.1)\nRequirement already satisfied: wrapt>=1.11.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\nCollecting numpy>=1.17\n  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\nRequirement already satisfied: termcolor>=1.1.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: astunparse>=1.6.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nCollecting keras>=3.5.0\n  Using cached keras-3.9.1-py3-none-any.whl (1.3 MB)\nRequirement already satisfied: absl-py>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.0.0)\nCollecting graphene<4\n  Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.4.39)\nCollecting docker<8,>=4.0.0\n  Using cached docker-7.1.0-py3-none-any.whl (147 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.3.4)\nRequirement already satisfied: gunicorn<24 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.5.2)\nCollecting alembic!=1.10.0,<2\n  Using cached alembic-1.15.1-py3-none-any.whl (231 kB)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.1.2+db1)\nCollecting mlflow-skinny==2.21.2\n  Using cached mlflow_skinny-2.21.2-py3-none-any.whl (6.1 MB)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (2.0.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (0.4.2)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (8.0.4)\nCollecting opentelemetry-sdk<3,>=1.9.0\n  Using cached opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (0.23.2)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.27)\nCollecting opentelemetry-api<3,>=1.9.0\n  Using cached opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (4.11.3)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow-skinny==2.21.2->mlflow) (0.98.0)\nCollecting cachetools<6,>=5.0.0\n  Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\nCollecting pydantic<3,>=1.10.8\n  Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\nCollecting databricks-sdk<1,>=0.20.0\n  Using cached databricks_sdk-0.48.0-py3-none-any.whl (677 kB)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.11)\nRequirement already satisfied: itsdangerous>=0.24 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow) (2.0.1)\nRequirement already satisfied: Werkzeug>=0.15 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow) (2.0.3)\nCollecting graphql-core<3.3,>=3.1\n  Using cached graphql_core-3.2.6-py3-none-any.whl (203 kB)\nCollecting graphql-relay<3.3,>=3.1\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.10/site-packages (from graphene<4->mlflow) (2.8.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch==2.6.0->torchaudio) (2.0.1)\nCollecting namex\n  Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\nCollecting rich\n  Using cached rich-13.9.4-py3-none-any.whl (242 kB)\nCollecting optree\n  Using cached optree-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (395 kB)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.2.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.25.0)\nCollecting numba>=0.51.0\n  Using cached numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\nCollecting llvmlite<0.45,>=0.44.0dev0\n  Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /databricks/python3/lib/python3.10/site-packages (from pooch>=1.0->librosa) (2.5.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2022.9.14)\nCollecting scipy>=1.2.0\n  Using cached scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\nRequirement already satisfied: cffi>=1.0 in /databricks/python3/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (1.1.1)\nCollecting grpcio<2.0,>=1.24.3\n  Using cached grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0\n  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\nCollecting google-auth~=2.0\n  Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\nCollecting pydantic<3,>=1.10.8\n  Using cached pydantic-1.10.21-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /databricks/python3/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==2.21.2->mlflow) (0.27.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (4.0.10)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.2->mlflow) (3.8.0)\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0\n  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\nCollecting deprecated>=1.2.6\n  Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\nCollecting opentelemetry-semantic-conventions==0.52b1\n  Using cached opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==2.21.2->mlflow) (0.14.0)\nCollecting markdown-it-py>=2.2.0\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nCollecting pygments<3.0.0,>=2.13.0\n  Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (5.0.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.2.8)\nCollecting mdurl~=0.1\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nRequirement already satisfied: anyio<5,>=3.4.0 in /databricks/python3/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (3.5.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (1.2.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.4.8)\nInstalling collected packages: triton, nvidia-cusparselt-cu12, namex, mpmath, flatbuffers, zipp, typing-extensions, tensorboard-data-server, sympy, pygments, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mdurl, llvmlite, grpcio, graphql-core, deprecated, cachetools, tensorboard, scipy, pydantic, optree, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, ml-dtypes, markdown-it-py, importlib_metadata, h5py, graphql-relay, google-auth, docker, alembic, rich, opentelemetry-api, nvidia-cusolver-cu12, graphene, databricks-sdk, torch, opentelemetry-semantic-conventions, keras, torchaudio, tensorflow, opentelemetry-sdk, mlflow-skinny, mlflow\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Not uninstalling flatbuffers at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'flatbuffers'. No files were found to uninstall.\n  Attempting uninstall: zipp\n    Found existing installation: zipp 3.8.0\n    Not uninstalling zipp at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'zipp'. No files were found to uninstall.\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: tensorboard-data-server\n    Found existing installation: tensorboard-data-server 0.6.1\n    Not uninstalling tensorboard-data-server at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'tensorboard-data-server'. No files were found to uninstall.\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.11.2\n    Not uninstalling pygments at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'Pygments'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.38.0\n    Not uninstalling llvmlite at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'llvmlite'. No files were found to uninstall.\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.48.1\n    Not uninstalling grpcio at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'grpcio'. No files were found to uninstall.\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 4.2.4\n    Not uninstalling cachetools at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'cachetools'. No files were found to uninstall.\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.11.0\n    Not uninstalling tensorboard at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'tensorboard'. No files were found to uninstall.\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.9.1\n    Not uninstalling scipy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'scipy'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: numba\n    Found existing installation: numba 0.55.1\n    Not uninstalling numba at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'numba'. No files were found to uninstall.\n  Attempting uninstall: importlib_metadata\n    Found existing installation: importlib-metadata 4.11.3\n    Not uninstalling importlib-metadata at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Not uninstalling h5py at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'h5py'. No files were found to uninstall.\n  Attempting uninstall: google-auth\n    Found existing installation: google-auth 1.33.0\n    Not uninstalling google-auth at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'google-auth'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.1.6\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: torch\n    Found existing installation: torch 1.13.1+cpu\n    Not uninstalling torch at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'torch'. No files were found to uninstall.\n  Attempting uninstall: keras\n    Found existing installation: keras 2.11.0\n    Not uninstalling keras at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'keras'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.5.0\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-680d148b-9315-4b4e-88c5-24272f34df58\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.14.3 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.2.0 requires numpy<1.24,>=1.16.0, but you have numpy 2.1.3 which is incompatible.\nydata-profiling 4.2.0 requires scipy<1.11,>=1.4.1, but you have scipy 1.15.2 which is incompatible.\ntorchvision 0.14.1+cpu requires torch==1.13.1, but you have torch 2.6.0 which is incompatible.\ntensorflow-cpu 2.11.1 requires keras<2.12,>=2.11.0, but you have keras 3.9.1 which is incompatible.\ntensorflow-cpu 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-cpu 2.11.1 requires tensorboard<2.12,>=2.11, but you have tensorboard 2.19.0 which is incompatible.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.1.1 which is incompatible.\nlangchain 0.0.217 requires numpy<2,>=1, but you have numpy 2.1.3 which is incompatible.\ngoogleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 5.29.4 which is incompatible.\ndatabricks-feature-store 0.14.3 requires numpy<2,>=1.19.2, but you have numpy 2.1.3 which is incompatible.\ndatabricks-feature-store 0.14.3 requires protobuf<5,>=3.12.0, but you have protobuf 5.29.4 which is incompatible.\nSuccessfully installed alembic-1.15.1 cachetools-5.5.2 databricks-sdk-0.48.0 deprecated-1.2.18 docker-7.1.0 flatbuffers-25.2.10 google-auth-2.38.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 grpcio-1.71.0 h5py-3.13.0 importlib_metadata-8.6.1 keras-3.9.1 llvmlite-0.44.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 mlflow-2.21.2 mlflow-skinny-2.21.2 mpmath-1.3.0 namex-0.0.8 numba-0.61.0 numpy-2.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opentelemetry-api-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 optree-0.14.1 protobuf-5.29.4 pydantic-1.10.21 pygments-2.19.1 rich-13.9.4 scipy-1.15.2 sympy-1.13.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 torch-2.6.0 torchaudio-2.6.0 triton-3.2.0 typing-extensions-4.13.0 zipp-3.21.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets librosa torchaudio scikit-learn tensorflow mlflow accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fd0227-228f-44fc-9b71-f3036b517b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\nWARNING:datasets.builder:Found cached dataset empathetic_dialogues (/root/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"empathetic_dialogues\", split=\"train\")\n",
    "\n",
    "# Extract prompt and context\n",
    "texts = [d['prompt'] for d in data]\n",
    "labels = [d['context'] for d in data]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode labels to numeric format\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Save mapping for later use\n",
    "label_to_id = {label: idx for idx, label in enumerate(le.classes_)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4d0343-357e-4fc4-882f-202c17169d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1854a7221c4d0cb73e1ae6a28e06ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df[['text', 'label_encoded']])\n",
    "hf_dataset = hf_dataset.rename_column(\"label_encoded\", \"label\")\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83af10b2-4739-4714-97bf-a65b988e0ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting typing_extensions==4.8.0\n  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\nInstalling collected packages: typing_extensions\n  Attempting uninstall: typing_extensions\n    Found existing installation: typing_extensions 4.13.0\n    Uninstalling typing_extensions-4.13.0:\n      Successfully uninstalled typing_extensions-4.13.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.14.3 requires pyspark<4,>=3.1.2, which is not installed.\ntorch 2.6.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.8.0 which is incompatible.\nalembic 1.15.1 requires typing-extensions>=4.12, but you have typing-extensions 4.8.0 which is incompatible.\nydata-profiling 4.2.0 requires numpy<1.24,>=1.16.0, but you have numpy 2.1.3 which is incompatible.\nydata-profiling 4.2.0 requires scipy<1.11,>=1.4.1, but you have scipy 1.15.2 which is incompatible.\ntorchvision 0.14.1+cpu requires torch==1.13.1, but you have torch 2.6.0 which is incompatible.\ntensorflow-cpu 2.11.1 requires keras<2.12,>=2.11.0, but you have keras 3.9.1 which is incompatible.\ntensorflow-cpu 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-cpu 2.11.1 requires tensorboard<2.12,>=2.11, but you have tensorboard 2.19.0 which is incompatible.\nlangchain 0.0.217 requires numpy<2,>=1, but you have numpy 2.1.3 which is incompatible.\ndatabricks-feature-store 0.14.3 requires numpy<2,>=1.19.2, but you have numpy 2.1.3 which is incompatible.\ndatabricks-feature-store 0.14.3 requires protobuf<5,>=3.12.0, but you have protobuf 5.29.4 which is incompatible.\nSuccessfully installed typing_extensions-4.8.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install typing_extensions==4.8.0 --force-reinstall --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0390d1f-e7fa-4b6a-a7a1-eda31237ea40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2381dce-ef0f-461b-88b5-95c4d375ad51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd9fdb685b74ece90aa525bab3ad106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Convert your pandas df to HF dataset\n",
    "hf_dataset = Dataset.from_pandas(df[['text', 'label_encoded']])\n",
    "hf_dataset = hf_dataset.rename_column(\"label_encoded\", \"label\")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Format for PyTorch\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43c20d0-f2ba-4545-b5ca-0eb465e41e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Use a smaller subset for faster training\n",
    "small_dataset = tokenized_dataset.shuffle(seed=42).select(range(1000))\n",
    "train_size = int(0.8 * len(small_dataset))\n",
    "\n",
    "train_dataset = small_dataset.select(range(train_size))\n",
    "eval_dataset = small_dataset.select(range(train_size, len(small_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c7635b-77da-4a78-ab79-2bfaf373156a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c023af52-39f4-4e11-b6d6-8114cd5f9cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\r  0%|          | 0/50 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/50 [00:26<?, ?it/s]\rEpoch 0:   0%|          | 0/50 [00:26<?, ?it/s, loss=3.48]\rEpoch 0:   2%|         | 1/50 [00:26<21:59, 26.93s/it, loss=3.48]\rEpoch 0:   2%|         | 1/50 [01:04<21:59, 26.93s/it, loss=3.48]\rEpoch 0:   2%|         | 1/50 [01:04<21:59, 26.93s/it, loss=3.49]\rEpoch 0:   4%|         | 2/50 [01:04<26:27, 33.07s/it, loss=3.49]\rEpoch 0:   4%|         | 2/50 [02:08<26:27, 33.07s/it, loss=3.49]\rEpoch 0:   4%|         | 2/50 [02:08<26:27, 33.07s/it, loss=3.43]\rEpoch 0:   6%|         | 3/50 [02:08<37:04, 47.32s/it, loss=3.43]\rEpoch 0:   6%|         | 3/50 [03:02<37:04, 47.32s/it, loss=3.43]\rEpoch 0:   6%|         | 3/50 [03:02<37:04, 47.32s/it, loss=3.43]\rEpoch 0:   8%|         | 4/50 [03:02<38:19, 50.00s/it, loss=3.43]\rEpoch 0:   8%|         | 4/50 [04:22<38:19, 50.00s/it, loss=3.43]\rEpoch 0:   8%|         | 4/50 [04:22<38:19, 50.00s/it, loss=3.46]\rEpoch 0:  10%|         | 5/50 [04:22<45:35, 60.78s/it, loss=3.46]\rEpoch 0:  10%|         | 5/50 [05:42<45:35, 60.78s/it, loss=3.46]\rEpoch 0:  10%|         | 5/50 [05:42<45:35, 60.78s/it, loss=3.5] \rEpoch 0:  12%|        | 6/50 [05:42<49:26, 67.42s/it, loss=3.5]\rEpoch 0:  12%|        | 6/50 [07:09<49:26, 67.42s/it, loss=3.5]\rEpoch 0:  12%|        | 6/50 [07:09<49:26, 67.42s/it, loss=3.44]\rEpoch 0:  14%|        | 7/50 [07:09<52:43, 73.58s/it, loss=3.44]\rEpoch 0:  14%|        | 7/50 [08:53<52:43, 73.58s/it, loss=3.44]\rEpoch 0:  14%|        | 7/50 [08:53<52:43, 73.58s/it, loss=3.44]\rEpoch 0:  16%|        | 8/50 [08:53<58:24, 83.43s/it, loss=3.44]\rEpoch 0:  16%|        | 8/50 [10:23<58:24, 83.43s/it, loss=3.44]\rEpoch 0:  16%|        | 8/50 [10:23<58:24, 83.43s/it, loss=3.47]\rEpoch 0:  18%|        | 9/50 [10:23<58:17, 85.30s/it, loss=3.47]\rEpoch 0:  18%|        | 9/50 [12:04<58:17, 85.30s/it, loss=3.47]\rEpoch 0:  18%|        | 9/50 [12:04<58:17, 85.30s/it, loss=3.51]\rEpoch 0:  20%|        | 10/50 [12:04<1:00:16, 90.41s/it, loss=3.51]\rEpoch 0:  20%|        | 10/50 [14:39<1:00:16, 90.41s/it, loss=3.51]\rEpoch 0:  20%|        | 10/50 [14:39<1:00:16, 90.41s/it, loss=3.57]\rEpoch 0:  22%|       | 11/50 [14:39<1:11:28, 109.95s/it, loss=3.57]\rEpoch 0:  22%|       | 11/50 [17:28<1:11:28, 109.95s/it, loss=3.57]\rEpoch 0:  22%|       | 11/50 [17:28<1:11:28, 109.95s/it, loss=3.44]\rEpoch 0:  24%|       | 12/50 [17:28<1:21:02, 127.96s/it, loss=3.44]\rEpoch 0:  24%|       | 12/50 [20:44<1:21:02, 127.96s/it, loss=3.44]\rEpoch 0:  24%|       | 12/50 [20:44<1:21:02, 127.96s/it, loss=3.45]\rEpoch 0:  26%|       | 13/50 [20:44<1:31:37, 148.57s/it, loss=3.45]\rEpoch 0:  26%|       | 13/50 [24:06<1:31:37, 148.57s/it, loss=3.45]\rEpoch 0:  26%|       | 13/50 [24:06<1:31:37, 148.57s/it, loss=3.48]\rEpoch 0:  28%|       | 14/50 [24:06<1:38:48, 164.68s/it, loss=3.48]\rEpoch 0:  28%|       | 14/50 [27:49<1:38:48, 164.68s/it, loss=3.48]\rEpoch 0:  28%|       | 14/50 [27:49<1:38:48, 164.68s/it, loss=3.43]\rEpoch 0:  30%|       | 15/50 [27:49<1:46:21, 182.32s/it, loss=3.43]\rEpoch 0:  30%|       | 15/50 [30:29<1:46:21, 182.32s/it, loss=3.43]\rEpoch 0:  30%|       | 15/50 [30:29<1:46:21, 182.32s/it, loss=3.43]\rEpoch 0:  32%|      | 16/50 [30:29<1:39:35, 175.75s/it, loss=3.43]\rEpoch 0:  32%|      | 16/50 [33:35<1:39:35, 175.75s/it, loss=3.43]\rEpoch 0:  32%|      | 16/50 [33:35<1:39:35, 175.75s/it, loss=3.5] \rEpoch 0:  34%|      | 17/50 [33:35<1:38:19, 178.76s/it, loss=3.5]\rEpoch 0:  34%|      | 17/50 [37:06<1:38:19, 178.76s/it, loss=3.5]\rEpoch 0:  34%|      | 17/50 [37:06<1:38:19, 178.76s/it, loss=3.4]\rEpoch 0:  36%|      | 18/50 [37:06<1:40:31, 188.50s/it, loss=3.4]\rEpoch 0:  36%|      | 18/50 [41:09<1:40:31, 188.50s/it, loss=3.4]\rEpoch 0:  36%|      | 18/50 [41:09<1:40:31, 188.50s/it, loss=3.46]\rEpoch 0:  38%|      | 19/50 [41:09<1:45:51, 204.88s/it, loss=3.46]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3876218e-cea4-430f-98b9-7d3849742ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\nWARNING:datasets.builder:Found cached dataset empathetic_dialogues (/root/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf)\nSome weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:03,  2.59it/s]\r 30%|       | 3/10 [00:00<00:01,  5.50it/s]\r 40%|      | 4/10 [00:00<00:00,  6.13it/s]\r 50%|     | 5/10 [00:00<00:00,  6.94it/s]\r 60%|    | 6/10 [00:00<00:00,  7.33it/s]\r 80%|  | 8/10 [00:01<00:00,  8.99it/s]\r 90%| | 9/10 [00:01<00:00,  8.42it/s]\r100%|| 10/10 [00:01<00:00,  8.78it/s]\r100%|| 10/10 [00:01<00:00,  7.29it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 20%|        | 2/10 [00:00<00:00, 10.14it/s]\r 40%|      | 4/10 [00:00<00:00,  9.94it/s]\r 50%|     | 5/10 [00:00<00:00,  9.93it/s]\r 60%|    | 6/10 [00:00<00:00,  9.57it/s]\r 80%|  | 8/10 [00:00<00:00,  8.77it/s]\r100%|| 10/10 [00:01<00:00,  9.10it/s]\r100%|| 10/10 [00:01<00:00,  9.30it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  8.49it/s]\r 20%|        | 2/10 [00:00<00:00,  8.30it/s]\r 30%|       | 3/10 [00:00<00:00,  7.52it/s]\r 40%|      | 4/10 [00:00<00:00,  8.25it/s]\r 50%|     | 5/10 [00:00<00:00,  7.89it/s]\r 60%|    | 6/10 [00:00<00:00,  8.39it/s]\r 70%|   | 7/10 [00:00<00:00,  8.51it/s]\r 80%|  | 8/10 [00:00<00:00,  8.89it/s]\r 90%| | 9/10 [00:01<00:00,  8.44it/s]\r100%|| 10/10 [00:01<00:00,  8.81it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.30it/s]\r 30%|       | 3/10 [00:00<00:00,  8.65it/s]\r 40%|      | 4/10 [00:00<00:00,  8.85it/s]\r 50%|     | 5/10 [00:00<00:00,  8.91it/s]\r 60%|    | 6/10 [00:00<00:00,  7.98it/s]\r 80%|  | 8/10 [00:00<00:00,  8.25it/s]\r 90%| | 9/10 [00:01<00:00,  8.39it/s]\r100%|| 10/10 [00:01<00:00,  8.53it/s]\r100%|| 10/10 [00:01<00:00,  8.50it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  7.77it/s]\r 20%|        | 2/10 [00:00<00:00,  8.81it/s]\r 30%|       | 3/10 [00:00<00:00,  7.03it/s]\r 40%|      | 4/10 [00:00<00:00,  6.25it/s]\r 50%|     | 5/10 [00:00<00:00,  7.08it/s]\r 60%|    | 6/10 [00:00<00:00,  7.83it/s]\r 70%|   | 7/10 [00:00<00:00,  7.23it/s]\r 80%|  | 8/10 [00:01<00:00,  7.88it/s]\r 90%| | 9/10 [00:01<00:00,  7.96it/s]\r100%|| 10/10 [00:01<00:00,  7.80it/s]\r100%|| 10/10 [00:01<00:00,  7.54it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 20%|        | 2/10 [00:00<00:00,  8.62it/s]\r 40%|      | 4/10 [00:00<00:00,  8.95it/s]\r 50%|     | 5/10 [00:00<00:00,  9.15it/s]\r 60%|    | 6/10 [00:00<00:00,  9.25it/s]\r 70%|   | 7/10 [00:00<00:00,  9.14it/s]\r 90%| | 9/10 [00:00<00:00,  9.59it/s]\r100%|| 10/10 [00:01<00:00,  8.92it/s]\r100%|| 10/10 [00:01<00:00,  9.04it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  7.60it/s]\r 20%|        | 2/10 [00:00<00:01,  6.83it/s]\r 30%|       | 3/10 [00:00<00:01,  6.98it/s]\r 40%|      | 4/10 [00:00<00:00,  7.00it/s]\r 60%|    | 6/10 [00:00<00:00,  8.39it/s]\r 70%|   | 7/10 [00:00<00:00,  8.50it/s]\r 80%|  | 8/10 [00:00<00:00,  8.75it/s]\r 90%| | 9/10 [00:01<00:00,  8.92it/s]\r100%|| 10/10 [00:01<00:00,  8.49it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.78it/s]\r 20%|        | 2/10 [00:00<00:00,  9.64it/s]\r 30%|       | 3/10 [00:00<00:00,  7.26it/s]\r 40%|      | 4/10 [00:00<00:00,  7.32it/s]\r 50%|     | 5/10 [00:00<00:00,  7.04it/s]\r 60%|    | 6/10 [00:00<00:00,  6.31it/s]\r 70%|   | 7/10 [00:01<00:00,  5.38it/s]\r 80%|  | 8/10 [00:01<00:00,  6.16it/s]\r 90%| | 9/10 [00:01<00:00,  6.04it/s]\r100%|| 10/10 [00:01<00:00,  5.92it/s]\r100%|| 10/10 [00:01<00:00,  6.38it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  8.05it/s]\r 20%|        | 2/10 [00:00<00:00,  8.47it/s]\r 30%|       | 3/10 [00:00<00:00,  8.11it/s]\r 40%|      | 4/10 [00:00<00:00,  7.96it/s]\r 50%|     | 5/10 [00:00<00:00,  8.04it/s]\r 60%|    | 6/10 [00:00<00:00,  8.06it/s]\r 80%|  | 8/10 [00:00<00:00,  9.15it/s]\r 90%| | 9/10 [00:01<00:00,  8.79it/s]\r100%|| 10/10 [00:01<00:00,  8.91it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 20%|        | 2/10 [00:00<00:00, 10.22it/s]\r 40%|      | 4/10 [00:00<00:00,  8.85it/s]\r 60%|    | 6/10 [00:00<00:00,  9.12it/s]\r 70%|   | 7/10 [00:00<00:00,  7.52it/s]\r 80%|  | 8/10 [00:00<00:00,  7.99it/s]\r 90%| | 9/10 [00:01<00:00,  8.42it/s]\r100%|| 10/10 [00:01<00:00,  8.75it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  7.94it/s]\r 20%|        | 2/10 [00:00<00:00,  8.61it/s]\r 30%|       | 3/10 [00:00<00:00,  8.40it/s]\r 40%|      | 4/10 [00:00<00:00,  7.71it/s]\r 50%|     | 5/10 [00:00<00:00,  7.77it/s]\r 60%|    | 6/10 [00:00<00:00,  7.92it/s]\r 80%|  | 8/10 [00:00<00:00,  8.12it/s]\r 90%| | 9/10 [00:01<00:00,  8.38it/s]\r100%|| 10/10 [00:01<00:00,  8.70it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.78it/s]\r 20%|        | 2/10 [00:00<00:01,  6.92it/s]\r 30%|       | 3/10 [00:00<00:01,  6.27it/s]\r 40%|      | 4/10 [00:00<00:00,  7.09it/s]\r 50%|     | 5/10 [00:00<00:00,  6.48it/s]\r 60%|    | 6/10 [00:00<00:00,  6.75it/s]\r 70%|   | 7/10 [00:01<00:00,  6.45it/s]\r 90%| | 9/10 [00:01<00:00,  7.59it/s]\r100%|| 10/10 [00:01<00:00,  7.57it/s]\r100%|| 10/10 [00:01<00:00,  7.15it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  5.82it/s]\r 20%|        | 2/10 [00:00<00:01,  7.29it/s]\r 30%|       | 3/10 [00:00<00:00,  7.95it/s]\r 40%|      | 4/10 [00:00<00:00,  7.74it/s]\r 50%|     | 5/10 [00:00<00:00,  8.14it/s]\r 60%|    | 6/10 [00:00<00:00,  8.25it/s]\r 80%|  | 8/10 [00:01<00:00,  7.58it/s]\r 90%| | 9/10 [00:01<00:00,  7.41it/s]\r100%|| 10/10 [00:01<00:00,  7.77it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.47it/s]\r 20%|        | 2/10 [00:00<00:00,  9.61it/s]\r 30%|       | 3/10 [00:00<00:00,  9.51it/s]\r 50%|     | 5/10 [00:00<00:00,  9.64it/s]\r 70%|   | 7/10 [00:00<00:00,  9.26it/s]\r 80%|  | 8/10 [00:00<00:00,  9.31it/s]\r 90%| | 9/10 [00:00<00:00,  9.40it/s]\r100%|| 10/10 [00:01<00:00,  9.07it/s]\r100%|| 10/10 [00:01<00:00,  9.27it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.70it/s]\r 20%|        | 2/10 [00:00<00:01,  7.31it/s]\r 30%|       | 3/10 [00:00<00:00,  7.68it/s]\r 40%|      | 4/10 [00:00<00:00,  8.01it/s]\r 50%|     | 5/10 [00:00<00:00,  6.92it/s]\r 60%|    | 6/10 [00:00<00:00,  6.62it/s]\r 70%|   | 7/10 [00:00<00:00,  7.17it/s]\r 80%|  | 8/10 [00:01<00:00,  6.38it/s]\r 90%| | 9/10 [00:01<00:00,  6.33it/s]\r100%|| 10/10 [00:01<00:00,  7.12it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  5.57it/s]\r 20%|        | 2/10 [00:00<00:01,  6.96it/s]\r 30%|       | 3/10 [00:00<00:01,  5.76it/s]\r 40%|      | 4/10 [00:00<00:01,  5.50it/s]\r 50%|     | 5/10 [00:00<00:00,  6.21it/s]\r 60%|    | 6/10 [00:00<00:00,  6.05it/s]\r 70%|   | 7/10 [00:01<00:00,  6.77it/s]\r 80%|  | 8/10 [00:01<00:00,  6.28it/s]\r 90%| | 9/10 [00:01<00:00,  6.84it/s]\r100%|| 10/10 [00:01<00:00,  6.72it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  4.52it/s]\r 20%|        | 2/10 [00:00<00:01,  5.52it/s]\r 30%|       | 3/10 [00:00<00:02,  2.97it/s]\r 40%|      | 4/10 [00:01<00:01,  3.64it/s]\r 50%|     | 5/10 [00:01<00:01,  4.52it/s]\r 70%|   | 7/10 [00:01<00:00,  5.80it/s]\r 90%| | 9/10 [00:01<00:00,  6.56it/s]\r100%|| 10/10 [00:01<00:00,  5.62it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 20%|        | 2/10 [00:00<00:01,  7.58it/s]\r 30%|       | 3/10 [00:00<00:01,  6.14it/s]\r 40%|      | 4/10 [00:00<00:00,  6.60it/s]\r 50%|     | 5/10 [00:00<00:00,  7.32it/s]\r 60%|    | 6/10 [00:00<00:00,  7.50it/s]\r 70%|   | 7/10 [00:00<00:00,  7.99it/s]\r 80%|  | 8/10 [00:01<00:00,  7.41it/s]\r 90%| | 9/10 [00:01<00:00,  6.98it/s]\r100%|| 10/10 [00:01<00:00,  7.37it/s]\r100%|| 10/10 [00:01<00:00,  7.22it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  8.05it/s]\r 20%|        | 2/10 [00:00<00:01,  5.40it/s]\r 30%|       | 3/10 [00:00<00:01,  6.38it/s]\r 40%|      | 4/10 [00:00<00:00,  6.88it/s]\r 50%|     | 5/10 [00:00<00:00,  6.51it/s]\r 60%|    | 6/10 [00:00<00:00,  7.07it/s]\r 70%|   | 7/10 [00:01<00:00,  6.87it/s]\r 80%|  | 8/10 [00:01<00:00,  7.01it/s]\r100%|| 10/10 [00:01<00:00,  9.10it/s]\r100%|| 10/10 [00:01<00:00,  7.52it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:02,  4.46it/s]\r 20%|        | 2/10 [00:00<00:01,  5.44it/s]\r 30%|       | 3/10 [00:00<00:01,  4.30it/s]\r 40%|      | 4/10 [00:01<00:01,  3.64it/s]\r 50%|     | 5/10 [00:01<00:01,  3.47it/s]\r 60%|    | 6/10 [00:01<00:00,  4.11it/s]\r 70%|   | 7/10 [00:01<00:00,  4.73it/s]\r 80%|  | 8/10 [00:01<00:00,  5.02it/s]\r 90%| | 9/10 [00:01<00:00,  5.83it/s]\r100%|| 10/10 [00:02<00:00,  6.11it/s]\r100%|| 10/10 [00:02<00:00,  4.86it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  6.24it/s]\r 20%|        | 2/10 [00:00<00:01,  7.88it/s]\r 30%|       | 3/10 [00:00<00:00,  7.03it/s]\r 50%|     | 5/10 [00:00<00:00,  8.12it/s]\r 70%|   | 7/10 [00:00<00:00,  8.92it/s]\r 80%|  | 8/10 [00:00<00:00,  9.00it/s]\r100%|| 10/10 [00:01<00:00,  9.93it/s]\r100%|| 10/10 [00:01<00:00,  8.93it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  5.82it/s]\r 20%|        | 2/10 [00:00<00:01,  7.47it/s]\r 30%|       | 3/10 [00:00<00:00,  7.68it/s]\r 40%|      | 4/10 [00:00<00:00,  8.05it/s]\r 50%|     | 5/10 [00:00<00:00,  8.48it/s]\r 60%|    | 6/10 [00:00<00:00,  6.87it/s]\r 70%|   | 7/10 [00:00<00:00,  7.39it/s]\r 80%|  | 8/10 [00:01<00:00,  7.12it/s]\r 90%| | 9/10 [00:01<00:00,  7.58it/s]\r100%|| 10/10 [00:01<00:00,  7.82it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.12it/s]\r 20%|        | 2/10 [00:00<00:01,  6.60it/s]\r 40%|      | 4/10 [00:00<00:00,  8.47it/s]\r 50%|     | 5/10 [00:00<00:00,  8.87it/s]\r 60%|    | 6/10 [00:00<00:00,  7.94it/s]\r 70%|   | 7/10 [00:00<00:00,  7.84it/s]\r 80%|  | 8/10 [00:01<00:00,  7.24it/s]\r 90%| | 9/10 [00:01<00:00,  7.83it/s]\r100%|| 10/10 [00:01<00:00,  8.17it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  5.18it/s]\r 20%|        | 2/10 [00:00<00:01,  6.61it/s]\r 30%|       | 3/10 [00:00<00:01,  6.21it/s]\r 50%|     | 5/10 [00:00<00:00,  7.80it/s]\r 60%|    | 6/10 [00:00<00:00,  8.14it/s]\r 70%|   | 7/10 [00:00<00:00,  8.34it/s]\r 80%|  | 8/10 [00:01<00:00,  8.23it/s]\r 90%| | 9/10 [00:01<00:00,  8.57it/s]\r100%|| 10/10 [00:01<00:00,  8.24it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  7.19it/s]\r 20%|        | 2/10 [00:00<00:01,  7.67it/s]\r 30%|       | 3/10 [00:00<00:01,  6.03it/s]\r 40%|      | 4/10 [00:00<00:00,  7.14it/s]\r 50%|     | 5/10 [00:00<00:00,  5.15it/s]\r 70%|   | 7/10 [00:01<00:00,  6.71it/s]\r 80%|  | 8/10 [00:01<00:00,  7.33it/s]\r 90%| | 9/10 [00:01<00:00,  6.77it/s]\r100%|| 10/10 [00:01<00:00,  7.11it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 20%|        | 2/10 [00:00<00:01,  7.50it/s]\r 30%|       | 3/10 [00:00<00:01,  6.34it/s]\r 40%|      | 4/10 [00:00<00:01,  3.32it/s]\r 60%|    | 6/10 [00:01<00:00,  4.84it/s]\r 70%|   | 7/10 [00:01<00:00,  5.14it/s]\r 80%|  | 8/10 [00:01<00:00,  5.69it/s]\r100%|| 10/10 [00:01<00:00,  7.06it/s]\r100%|| 10/10 [00:01<00:00,  5.80it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  5.00it/s]\r 20%|        | 2/10 [00:00<00:01,  6.94it/s]\r 30%|       | 3/10 [00:00<00:00,  7.13it/s]\r 40%|      | 4/10 [00:00<00:00,  7.95it/s]\r 50%|     | 5/10 [00:00<00:00,  7.22it/s]\r 60%|    | 6/10 [00:00<00:00,  7.65it/s]\r 80%|  | 8/10 [00:01<00:00,  8.81it/s]\r 90%| | 9/10 [00:01<00:00,  8.78it/s]\r100%|| 10/10 [00:01<00:00,  9.00it/s]\r100%|| 10/10 [00:01<00:00,  8.11it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.37it/s]\r 20%|        | 2/10 [00:00<00:01,  7.86it/s]\r 30%|       | 3/10 [00:00<00:00,  7.21it/s]\r 40%|      | 4/10 [00:00<00:00,  7.91it/s]\r 50%|     | 5/10 [00:00<00:00,  7.19it/s]\r 60%|    | 6/10 [00:00<00:00,  7.68it/s]\r 80%|  | 8/10 [00:00<00:00,  8.46it/s]\r 90%| | 9/10 [00:01<00:00,  8.67it/s]\r100%|| 10/10 [00:01<00:00,  8.53it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:00,  9.96it/s]\r 20%|        | 2/10 [00:00<00:00,  8.81it/s]\r 30%|       | 3/10 [00:00<00:00,  8.48it/s]\r 40%|      | 4/10 [00:00<00:00,  8.42it/s]\r 50%|     | 5/10 [00:00<00:00,  7.98it/s]\r 60%|    | 6/10 [00:00<00:00,  7.62it/s]\r 70%|   | 7/10 [00:00<00:00,  7.54it/s]\r 80%|  | 8/10 [00:01<00:00,  7.21it/s]\r 90%| | 9/10 [00:01<00:00,  7.08it/s]\r100%|| 10/10 [00:01<00:00,  8.08it/s]\n\r  0%|          | 0/10 [00:00<?, ?it/s]\r 10%|         | 1/10 [00:00<00:01,  5.38it/s]\r 20%|        | 2/10 [00:00<00:01,  6.37it/s]\r 30%|       | 3/10 [00:00<00:01,  6.47it/s]\r 40%|      | 4/10 [00:00<00:00,  6.98it/s]\r 50%|     | 5/10 [00:00<00:00,  6.84it/s]\r 60%|    | 6/10 [00:00<00:00,  7.19it/s]\r 70%|   | 7/10 [00:01<00:00,  6.47it/s]\r 90%| | 9/10 [00:01<00:00,  6.61it/s]\r100%|| 10/10 [00:01<00:00,  6.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset(\"empathetic_dialogues\", split=\"train\")\n",
    "texts = [x['prompt'] for x in data.select(range(300))]\n",
    "labels = [x['context'] for x in data.select(range(300))]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "id_to_label = {i: label for i, label in enumerate(le.classes_)}\n",
    "\n",
    "# Load tokenizer & model\n",
    "model_name = \"prajjwal1/bert-tiny\"  # Fast lightweight model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
    "\n",
    "# Proper batch tokenization\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "labels_tensor = torch.tensor(encoded_labels)\n",
    "\n",
    "# Prepare dataset & dataloader\n",
    "dataset = TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], labels_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "model.train()\n",
    "\n",
    "# Training loop (1 epoch)\n",
    "for epoch in range(30):\n",
    "    for batch in tqdm(loader):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f77e80-9ca7-47c8-bbe8-46af07bfbfe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'afraid'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_text_emotion(text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    return id_to_label[pred]\n",
    "\n",
    "# Test\n",
    "detect_text_emotion(\"I feel exhausted and empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5ebf41-5bd2-42bf-8582-cc9a115fb2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f356aa133f4ea3acacdc853892146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8745d584c4df408e802fac634c6a3ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33012acc5b542f5b920adee147e5833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fecc83468a7475aa5431119ef0df76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9a7611e8d94c45b2bd472776b8c607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9acd5248854ffa8172c152ebfd1ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a small but effective dialogue model\n",
    "reply_gen = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad97b7e1-e422-40e3-bf79-b9f8434b6d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_empathetic_reply(user_text, emotion):\n",
    "    prompt = (\n",
    "        \"You are a caring and supportive AI friend who always replies with empathy and kindness.\\n\"\n",
    "        f\"The user feels {emotion}.\\n\"\n",
    "        f\"The user says: \\\"{user_text}\\\"\\n\"\n",
    "        \"Your empathetic response:\"\n",
    "    )\n",
    "\n",
    "    response = reply_gen(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # Extract meaningful part after \"empathetic response:\"\n",
    "    if \"empathetic response:\" in response:\n",
    "        reply = response.split(\"empathetic response:\")[-1].strip()\n",
    "    else:\n",
    "        reply = response.strip()\n",
    "\n",
    "    # Fallback if model generates a junk value\n",
    "    if reply == \"\" or reply.lower() in [\"1\", \"yes\", \"no\", \"okay\"]:\n",
    "        reply = \"Im really sorry youre feeling this way. Youre not alone, and things will get better. I'm here for you.\"\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6295a91d-f0a4-4856-ac4b-3bf446d8b2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def full_empathetic_pipeline(user_text):\n",
    "    emotion = detect_text_emotion(user_text)\n",
    "    reply = generate_empathetic_reply(user_text, emotion)\n",
    "\n",
    "    return {\n",
    "        \"User Input\": user_text,\n",
    "        \"Detected Emotion\": emotion,\n",
    "        \"Empathetic Reply\": reply\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbbbcb1-8db5-4867-8c6a-4ca021ea12b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: angry\nReply: Im really sorry youre feeling this way. Youre not alone, and things will get better. I'm here for you.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"I feel like everyone has left me behind.\"\n",
    "emotion = detect_text_emotion(text_input)\n",
    "reply = generate_empathetic_reply(text_input, emotion)\n",
    "\n",
    "print(\"Emotion:\", emotion)\n",
    "print(\"Reply:\", reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0763b8f2-cb87-4e85-984a-ae7b2551b0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "class MindMateModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        user_text = model_input[\"text\"][0]\n",
    "        emotion = detect_text_emotion(user_text)\n",
    "        reply = generate_empathetic_reply(user_text, emotion)\n",
    "        return [[emotion, reply]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ce4ba5-ee63-4873-bdb4-a38bc9e44855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/27 14:03:37 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.5.0/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Run ID: 9c70e8444e554086b08724a0ccb3b58b\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "#  End existing run if active\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "#  Now start a fresh run\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"mindmate_model\",\n",
    "        python_model=MindMateModel()\n",
    "    )\n",
    "    run_id = run.info.run_id\n",
    "    print(\" Run ID:\", run_id)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MindMate_Backend",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}